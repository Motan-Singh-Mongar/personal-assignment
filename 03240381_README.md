Q-learning on GridWorld: The game is to have a Q-learning to train an agent to navigate a 5x5 GridWorld, where an agent has to move out of a start point (0, 0 ) to a goal point (4, 4 ) without hitting a wall. Q-learning, a model-free reinforcement learning algorithm, is another method that is employed to learn an optimal policy by updating a Q-table via trial and error where the agent is rewarded depending on the action that it takes. The goal of the agent is to maximize its cumulative reward by reconciling exploration (trial random actions) and exploitation (choosing the best-known actions).

solution overview: Q-learning was used to solve the 5 x 5 GridWorld problem in this assignment and the agent needed to move through the world starting at a start (the top-left) and moving to the goal (the bottom-right) without hitting the walls. At each step the agent had four possible actions (up, down, left, right), the Q-table was updated with the following formula: Q(s,a)‚ÜêQ(s,a) +alpha(r+y(max) 3a Q(s^,a)-Q(s,a).

The parameter used was 1000 episodes with alpha=0.1, gamma=0.99 and epsilon=0.1. The acquired policy was pictorialized demonstrating the optimal activity in every state in the grid. The agent was able to reach the average of 9.03 out of 10 in the last 100 episodes by the end of the training. During testing, the agent was able to reach the goal 100 percent of the time with an average of 8 steps to the goal. The Q-table contained 80 non-zero values with the highest value of Q being 10.00 which means that the robot learns well and navigates.

Deep Q-Network (DQN) of cartpole: The objective here is to train a Deep Q-network (DQN) in order to play the CartPole-v1 game whereby the task is to balance a pole on a cart by making discrete actions (left or right) using the state of the environment. DQN is also a technique in reinforcement learning that involves the use of a neural network to estimate the Q-value function which controls the decisions of the agent. The agent is trained by experience with the help of a replay buffer and an epsilon-greedy approach towards exploration and exploitation.

solution overview: The solution uses a Deep Q-Network (DQN) to play the CartPole-v1 environment. The DQN is composed of a pair of hidden layers consisting of 128 units of a neural network to estimate the Q-function. The agent employs an epsilon-greedy action plan where the first stage is exploration and the second stage is exploitation of the learned Q-values. Transitions are stored and sampled by experience replay, so that training is stabilized. To stabilize predictions of Q-values, a target network is regularly updated. The agent undergoes training across 500 episodes, and training is terminated once an average reward of the final 100 episodes is greater than 475. The agent is then tested after the training on 10 episodes and its performance is measured. The method illustrates how DQN can successfully learn to solve the CartPole-v1 problem through the reinforcement learning approach.

Comparing and debugging Q-learning: The task consists in diagnosing and fixing bugs in a Q-learning code to solve the FrozenLake-v1 environment. These were wrong Q-table initialisation, wrong action selection (minimization of Q-values rather than maximization), incorrect Q-learning update rule (learns rate absent), and exploration-exploitation balance (no epsilon decay used).

solution overview: The solution tries to fix four critical bugs in a Q-learning implementation of Frozen Lake-v1 environment. First, the Q-table was filled in correctly, but it had to be checked on the dimensions. Second, the action selection mechanism was incorrect, and it selected the worst action with np.argmin (it should have been the best action with np.argmax). Third, the update rule of Q-learning lacked a learning rate (alpha) and therefore Q-values were overwritten instead of updated gradually. Lastly, epsilon decay was absent and as such, the agent remained stuck in the exploration stage and was unable to use the knowledge it learned. With these fixes, the performance of the agent was vastly improved with a success rate of 0.33 and 5000 episodes compared to 0.0 with the buggy version. The improvement was well demonstrated in a comparison plot and demonstrates the significant role of selecting the correct action, updating the Q-values and effective balance between exploring and exploiting.
